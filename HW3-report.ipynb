{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "#### submitted by: Amin Shojaeighadikolaei   Mar-24-2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question: Implement a Multilayer Perceptron model with the Backpropagation algorithm using only NumPy and no other module.\n",
    "\n",
    "- data is the same as in Homework 2\n",
    "- 3 layers, 800 neurons in each hidden layer, ReLU activations and softmax in the last layer\n",
    "- batchsize=30 and shuffle before each epoch\n",
    "- test as in Homework 2 and show the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we want to design a Neural Network with Backpropagation algorithm in which we just using Numpy module. Based on the question, the Network is 3 layer which means has 2 hidden layer each has 800 neurons. The dataset is mnist148.nbz which is consist of 300 sample of handwriting images and 3 image for test. In hence, the input size is (300*28*28) , output size is (300*1) and test size is (3*28*28). At first I want to import the libraries and then I will write the functions for Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, layers_size):\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    "        self.once = True\n",
    " \n",
    " \n",
    "    def activation_function(self,type, x , derivation):\n",
    "        if type == \"Sigmoid\" :\n",
    "            if derivation == False:\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "            else:\n",
    "                s = 1 / (1 + np.exp(-x))\n",
    "                return s * (1 - s)\n",
    "\n",
    "        if type == \"ReLU\":\n",
    "            if derivation == False:\n",
    "                return x * (x > 0)\n",
    "            else:\n",
    "                return 1. * (x > 0)\n",
    "\n",
    "        if type == \"Softmax\":\n",
    "            expZ = np.exp(x - np.max(x))\n",
    "            return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.activation_function(\"ReLU\" ,Z, derivation=False)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.activation_function( \"Softmax\",Z , derivation= False)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    " \n",
    "        return A, store\n",
    " \n",
    "   \n",
    " \n",
    "    def backward(self, X, Y, store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    " \n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    " \n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.activation_function( \"ReLU\", store[\"Z\" + str(l)] , derivation = True)\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    " \n",
    "        return derivatives\n",
    " \n",
    "    def train(self, X, Y, learning_rate=0.01, n_iterations=1):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    "        \n",
    "        if self.once == True :\n",
    "            self.layers_size.insert(0, X.shape[1])\n",
    "            self.once = False\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            derivatives = self.backward(X, Y, store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size is:  (300, 28, 28)\n",
      "Output size is:  (300,)\n",
      "Test size is:  (3, 28, 28)\n",
      "the first 10 samples of output is:  [4 1 1 1 4 1 8 4 1 1]\n"
     ]
    }
   ],
   "source": [
    "dataset= np.load(r'C:\\Users\\a335s717\\Desktop\\HW2\\mnist148.npz')\n",
    "new_dataset= dataset.files\n",
    "X = dataset['arr_0']\n",
    "Y = dataset['arr_1']\n",
    "Test = dataset['arr_2']\n",
    "\n",
    "print(\"Input size is: \", X.shape)\n",
    "print(\"Output size is: \", Y.shape)\n",
    "print(\"Test size is: \", Test.shape)\n",
    "print(\"the first 10 samples of output is: \", Y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is preparing the dataset. \n",
    "- the first thing is the size of the input which is (28*28) for each sample and we should change it to 784.\n",
    "- Next is the output value which is 1,4 or 8. this is categorical output and we have to use onehot encoding method to change the output.\n",
    "- Normalizing the input and output is the last thing that I want to do for preparing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input size: (300, 784)\n",
      "Train Output size: (300, 3)\n",
      "Test Input size: (3, 784)\n",
      " the first 10 sample for output is:  [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Input = []\n",
    "Output=[]\n",
    "count = np.zeros((10))\n",
    "w = np.random.random((28 * 28, 3))\n",
    "for x, y in zip(X,Y):\n",
    "    if y in [1, 4, 8]:\n",
    "        Input.append(x.reshape((28 * 28)) / 255)\n",
    "        count[y] += 1\n",
    "        \n",
    "        if y == [1]:\n",
    "            y = [1, 0, 0]\n",
    "        elif y == [4]:\n",
    "            y = [0, 1, 0]\n",
    "        elif y == [8]:\n",
    "            y = [0, 0, 1]   \n",
    "        Output.append(y)\n",
    "x_test=[]\n",
    "for x in Test : # reshape and normalize data\n",
    "    x_test.append(x.reshape((28 * 28)) / 255)\n",
    "\n",
    "samples = np.asarray(Input)\n",
    "labels = np.asarray(Output)\n",
    "test = np.asarray(x_test)\n",
    "\n",
    "X_train = samples\n",
    "Y_train = labels\n",
    "X_Test = test\n",
    "\n",
    "print(\"train input size: \" + str(X_train.shape))\n",
    "print(\"Train Output size: \"+ str(Y_train.shape))\n",
    "print(\"Test Input size: \" + str(X_Test.shape))\n",
    "print(\" the first 10 sample for output is: \", Y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is prepared and I want to use the functions of the Neural Network to create a NN for the dataset. At first I define the number of layers and nodes. Based on the question, the NN should be 3 layers which means 1 input layer, 2 hidden layer( 800 nodes in each) and 1 output layer( which is 3 nodes- because the out put is 3 base on one hot coding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_nodes = [800 ,800, 3] # how many nodes for hidden layers and output layer is needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ANN(layers_nodes)    # Create the Neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Neural network is created and it consists of 784 nodes for input, 800 nodes for first hidden layer, 800 nodes for second hidden layer and 3 nodes for output. In next I want to train the Neural network but before that based on the question I hava to shuffle the dataset and select a batch of data instead of training the dataset with the all 300 samples. in hence, 3 parameters have to be defined. \n",
    "\n",
    "- Epoch: One epoch is when all data samples train through NN.\n",
    "- iteration: number of iteration is number of passes, each pass using [batchsize] number of examples.\n",
    "- Batchsize: The number of training examples in one forward/backward pass.\n",
    "\n",
    "Just note that before each epoch the dataset will shuffle and then 10 batchsize will select in which each batchsize is consisting of 30 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Cost:  0.3647747973326185 Train Accuracy: 50.0\n",
      "Cost:  0.37351428400254233 Train Accuracy: 53.333333333333336\n",
      "Cost:  0.37449102210229523 Train Accuracy: 73.33333333333333\n",
      "Cost:  0.3651801345834054 Train Accuracy: 43.333333333333336\n",
      "Cost:  0.3699372168579389 Train Accuracy: 73.33333333333333\n",
      "Cost:  0.3636233866144339 Train Accuracy: 73.33333333333333\n",
      "Cost:  0.36651467893414796 Train Accuracy: 73.33333333333333\n",
      "Cost:  0.3699229462128367 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.364851742303374 Train Accuracy: 60.0\n",
      "Cost:  0.3673345729895209 Train Accuracy: 50.0\n",
      "epoch:  1\n",
      "Cost:  0.369788738247831 Train Accuracy: 63.33333333333333\n",
      "Cost:  0.37023919177802755 Train Accuracy: 60.0\n",
      "Cost:  0.36878824710470737 Train Accuracy: 86.66666666666667\n",
      "Cost:  0.3719191449304808 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.3692957116891771 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.37047784365167963 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.37053103310769514 Train Accuracy: 56.666666666666664\n",
      "Cost:  0.3679490299115552 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.3601205502841037 Train Accuracy: 70.0\n",
      "Cost:  0.36103529122785627 Train Accuracy: 43.333333333333336\n",
      "epoch:  2\n",
      "Cost:  0.3650294060421056 Train Accuracy: 56.666666666666664\n",
      "Cost:  0.36501982885971085 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.3757946193164664 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.3580129238431909 Train Accuracy: 66.66666666666666\n",
      "Cost:  0.37673734502680545 Train Accuracy: 43.333333333333336\n",
      "Cost:  0.37062661779900136 Train Accuracy: 80.0\n",
      "Cost:  0.37175553211734946 Train Accuracy: 80.0\n",
      "Cost:  0.36233268436466204 Train Accuracy: 90.0\n",
      "Cost:  0.36977417696413084 Train Accuracy: 90.0\n",
      "Cost:  0.36506164759969095 Train Accuracy: 73.33333333333333\n",
      "epoch:  3\n",
      "Cost:  0.37510514087986263 Train Accuracy: 63.33333333333333\n",
      "Cost:  0.3742029866791403 Train Accuracy: 70.0\n",
      "Cost:  0.35989155767906295 Train Accuracy: 60.0\n",
      "Cost:  0.37371413202851583 Train Accuracy: 46.666666666666664\n",
      "Cost:  0.3660113852779359 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.36602772175489046 Train Accuracy: 66.66666666666666\n",
      "Cost:  0.3698132040688556 Train Accuracy: 53.333333333333336\n",
      "Cost:  0.35349630809360133 Train Accuracy: 56.666666666666664\n",
      "Cost:  0.3662425020975664 Train Accuracy: 70.0\n",
      "Cost:  0.3756398433736823 Train Accuracy: 96.66666666666667\n",
      "epoch:  4\n",
      "Cost:  0.3699702110159891 Train Accuracy: 90.0\n",
      "Cost:  0.3701808037956852 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.36720503858093745 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.3680362052951365 Train Accuracy: 80.0\n",
      "Cost:  0.3716430371709486 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.3671947128303307 Train Accuracy: 46.666666666666664\n",
      "Cost:  0.37529784511640496 Train Accuracy: 93.33333333333333\n",
      "Cost:  0.3559952774358265 Train Accuracy: 40.0\n",
      "Cost:  0.36816031856112247 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.36646133213073234 Train Accuracy: 63.33333333333333\n",
      "epoch:  5\n",
      "Cost:  0.3611524154095015 Train Accuracy: 56.666666666666664\n",
      "Cost:  0.3628655528379406 Train Accuracy: 50.0\n",
      "Cost:  0.36300754116692374 Train Accuracy: 50.0\n",
      "Cost:  0.36710022027812844 Train Accuracy: 66.66666666666666\n",
      "Cost:  0.37276007626723284 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.3773003835388816 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.3723435824193378 Train Accuracy: 43.333333333333336\n",
      "Cost:  0.3593656216267217 Train Accuracy: 73.33333333333333\n",
      "Cost:  0.37325806588730825 Train Accuracy: 46.666666666666664\n",
      "Cost:  0.37099132250113714 Train Accuracy: 93.33333333333333\n",
      "epoch:  6\n",
      "Cost:  0.37637557458822224 Train Accuracy: 86.66666666666667\n",
      "Cost:  0.36842132851918696 Train Accuracy: 70.0\n",
      "Cost:  0.3665848317740462 Train Accuracy: 96.66666666666667\n",
      "Cost:  0.3632884020015173 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.3718740635484883 Train Accuracy: 50.0\n",
      "Cost:  0.36510650809819983 Train Accuracy: 46.666666666666664\n",
      "Cost:  0.3681258349740874 Train Accuracy: 66.66666666666666\n",
      "Cost:  0.36558578412721593 Train Accuracy: 66.66666666666666\n",
      "Cost:  0.3696009909941606 Train Accuracy: 80.0\n",
      "Cost:  0.3651814633079892 Train Accuracy: 83.33333333333334\n",
      "epoch:  7\n",
      "Cost:  0.37144076674722676 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.37398247953298397 Train Accuracy: 76.66666666666667\n",
      "Cost:  0.3610366540577523 Train Accuracy: 70.0\n",
      "Cost:  0.3661820379495419 Train Accuracy: 63.33333333333333\n",
      "Cost:  0.35601799746325025 Train Accuracy: 43.333333333333336\n",
      "Cost:  0.36931496052607277 Train Accuracy: 56.666666666666664\n",
      "Cost:  0.36210059551569584 Train Accuracy: 63.33333333333333\n",
      "Cost:  0.3779548680600507 Train Accuracy: 73.33333333333333\n",
      "Cost:  0.37611362013681277 Train Accuracy: 73.33333333333333\n",
      "Cost:  0.3660008019437262 Train Accuracy: 53.333333333333336\n",
      "epoch:  8\n",
      "Cost:  0.3676225684001882 Train Accuracy: 63.33333333333333\n",
      "Cost:  0.3687747692549305 Train Accuracy: 96.66666666666667\n",
      "Cost:  0.3726490056779473 Train Accuracy: 100.0\n",
      "Cost:  0.37223016995745106 Train Accuracy: 93.33333333333333\n",
      "Cost:  0.3627074324709501 Train Accuracy: 50.0\n",
      "Cost:  0.37364443343539444 Train Accuracy: 93.33333333333333\n",
      "Cost:  0.3570072153381572 Train Accuracy: 63.33333333333333\n",
      "Cost:  0.3807741262321937 Train Accuracy: 50.0\n",
      "Cost:  0.36213983418988094 Train Accuracy: 53.333333333333336\n",
      "Cost:  0.3625952269760202 Train Accuracy: 46.666666666666664\n",
      "epoch:  9\n",
      "Cost:  0.36518813224336966 Train Accuracy: 46.666666666666664\n",
      "Cost:  0.370023197082366 Train Accuracy: 93.33333333333333\n",
      "Cost:  0.37126875194680653 Train Accuracy: 83.33333333333334\n",
      "Cost:  0.3726002371284105 Train Accuracy: 90.0\n",
      "Cost:  0.3556982417287625 Train Accuracy: 50.0\n",
      "Cost:  0.3737162880128007 Train Accuracy: 53.333333333333336\n",
      "Cost:  0.36428378463319 Train Accuracy: 63.33333333333333\n",
      "Cost:  0.37355706346295847 Train Accuracy: 80.0\n",
      "Cost:  0.3668766759170769 Train Accuracy: 66.66666666666666\n",
      "Cost:  0.36693240977737257 Train Accuracy: 70.0\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "iteration = 100\n",
    "batchsize = 30\n",
    "\n",
    "for i in range(epoch):  # number of epoch\n",
    "    print(\"epoch: \", i)\n",
    "\n",
    "    # Shuffle dataset before going to trainig\n",
    "    s = np.arange(X_train.shape[0]) \n",
    "    np.random.shuffle(s)\n",
    "    X_train= X_train[s]\n",
    "    Y_train = Y_train[s]\n",
    "    zzz = X_train.shape[0]\n",
    "\n",
    "    for i in range(0, X_train.shape[0],batchsize): # select a batch of data \n",
    "        XX_train = X_train[i:i+batchsize]\n",
    "        YY_train = Y_train[i:i+batchsize]\n",
    "        ann.train(XX_train, YY_train, learning_rate=0.1, n_iterations = iteration)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 91.33333333333333\n",
      "Predicted output: [[9.97249387e-01 6.93837154e-03 4.62578470e-02]\n",
      " [3.12329122e-04 9.37567067e-01 5.48170766e-03]\n",
      " [2.43828347e-03 5.54945616e-02 9.48260445e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy:\", ann.predict(X_train, Y_train))\n",
    "# print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "test , storage = ann.forward(X_Test)\n",
    "print(\"Predicted output: \"+ str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class for sample 1 is:  1\n",
      "predicted class for sample 2 is:  4\n",
      "predicted class for sample 3 is:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in range(3):\n",
    "    plt.subplot(2,2,c+1)\n",
    "    plt.imshow(Test[c])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Sample test'+str(c+1))\n",
    "    \n",
    "for i in range(3):\n",
    "    if np.argmax(test[i]) == 0: print(\"predicted class for sample \" + str(i+1) + ' is: ',  1)\n",
    "    if np.argmax(test[i]) == 1: print('predicted class for sample ' +str(i+1)+ ' is: ',  4)\n",
    "    if np.argmax(test[i]) == 2: print('predicted class for sample ' +str(i+1)+ ' is: ',  8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
